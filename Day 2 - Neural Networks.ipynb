{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "A Neural Network in a powerful classification tool and one of the main algorithms that has got so many people excited about Machine Learning as of late. It takes inspiration from the hierarchical structure of the visual processing systems found in many animals including humans.<br>\n",
    "\n",
    "It consists of layers of nodes, each node in each layer connected with each node in the previous layer with a weighted edge. The first layer is called the input layer, the last layer is called the output layer and any layers in between are called hidden layers.\n",
    "\n",
    "![](nn.png)\n",
    "\n",
    "Each node performs a very simple calculation. On a forward pass, where you feed in values into your input layer, each node calculates a weighted sum/linear combination of the nodes in the previous layer and applies an activation function. This is done sequentially through all the layers to get your output. The weights that each node uses to calculate its output are the parameters of the modela and they need to be optimised in order to the required output.\n",
    "\n",
    "There are many different activation functions which have been shown to work including sigmoid, tanh, RELU, ELU and more are being researched everyday. RELU is the most commonly used one. But for classification problems, a sigmoid activation is used on the final layer to limit the output between 0 and 1 to represent a probability.\n",
    "\n",
    "![](activation.png)\n",
    "\n",
    "To train these networks, we use good old gradient descent. We feed in the input for each data point we have to calculate our output. We use our output to calculate the loss and then calculate the derivative of our loss w.r.t each weight in the network. We update our weights and we perform the whole process iteratively until we are satisfied with how low our loss is. Gradient descent applied to optimizing a neural network is commonly known as back-propagation.\n",
    "\n",
    "What tends to happen with these networks one they are trained, especially when deeper architectures are used is that the first layers find low level features from the inputs. In the case of image processing, the neurons in the first layers activate for different types of lines and curves in the image while the the middle layers activate for different combinations of the lower level features such as basic shapes. The final layers activate for even higher level features such as (if we're tring to classify faces) ears, eyes and noses. And of course the neurons in the final layer activate for the higher level features we have which are different faces. This is exactly the kind of representations which are built up in biological visual processing systems.\n",
    "\n",
    "![](nnfeaturevis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be building a neural network which takes in as input 30 features which are different measurements taken from a tumor and outputs the probability of it being malignant. If a tumor is malignant it means that it is cancerous so you can see how useful this kind of model could be in the medical profession to assist with decision making.\n",
    "\n",
    "First, we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a Pandas dataframe, map our class labels into numerical values and shuffle our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "434   908469          1        14.86         16.94           94.89      673.7   \n",
      "328  8953902          0        16.27         20.71          106.90      813.7   \n",
      "269  8910720          1        10.71         20.39           69.50      344.9   \n",
      "310   893783          1        11.70         19.11           74.33      418.7   \n",
      "355  9010258          1        12.56         19.07           81.92      485.8   \n",
      "\n",
      "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "434          0.08924           0.07074         0.03346              0.02877   \n",
      "328          0.11690           0.13190         0.14780              0.08488   \n",
      "269          0.10820           0.12890         0.08448              0.02867   \n",
      "310          0.08814           0.05253         0.01583              0.01148   \n",
      "355          0.08760           0.10380         0.10300              0.04391   \n",
      "\n",
      "        ...       texture_worst  perimeter_worst  area_worst  \\\n",
      "434     ...               20.54           102.30       777.5   \n",
      "328     ...               30.38           129.80      1121.0   \n",
      "269     ...               25.21            76.51       410.4   \n",
      "310     ...               26.55            80.92       483.1   \n",
      "355     ...               22.43            89.02       547.4   \n",
      "\n",
      "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "434            0.1218             0.1550          0.12200   \n",
      "328            0.1590             0.2947          0.35970   \n",
      "269            0.1335             0.2550          0.25340   \n",
      "310            0.1223             0.1087          0.07915   \n",
      "355            0.1096             0.2002          0.23880   \n",
      "\n",
      "     concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
      "434               0.07971          0.2525                  0.06827   \n",
      "328               0.15830          0.3103                  0.08200   \n",
      "269               0.08600          0.2605                  0.08701   \n",
      "310               0.05741          0.3487                  0.06958   \n",
      "355               0.09265          0.2121                  0.07188   \n",
      "\n",
      "     Unnamed: 32  \n",
      "434          NaN  \n",
      "328          NaN  \n",
      "269          NaN  \n",
      "310          NaN  \n",
      "355          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cancer_data.csv')\n",
    "df[['diagnosis']] = df['diagnosis'].map({'M': 0, 'B': 1}) #map into numeric data\n",
    "df = df.sample(frac=1) #shuffle dataset\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the required data from the dataframe into torch tensors so we can use them in our model. Normalise input features to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(np.array(df[df.columns[2:-1]])) #pick our features from our dataset\n",
    "Y = torch.Tensor(np.array(df[['diagnosis']])) #select out label\n",
    "\n",
    "X = (X-X.mean(0)) / X.std(0) #normalize our features along the 0th axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose how many datapoints we will use as our training set and split intro train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 450\n",
    "\n",
    "x_train = Variable(X[:m])\n",
    "y_train = Variable(Y[:m])\n",
    "\n",
    "x_test = Variable(X[m:])\n",
    "y_test = Variable(Y[m:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Neural Network model using PyTorch's class interface. We define the Net class and inherit from torch.nn.module.<br>\n",
    "We define the required linear combination operations in the initializer function.<br>\n",
    "We define the forward-propagation, applying the linear combination operations defined in the initializer function followed by an activation function at each layer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #call parent class initializer\n",
    "        self.h1 = torch.nn.Linear(30, 10) #input layer to size 10 hidden layer\n",
    "        self.out = torch.nn.Linear(10, 1) #hidden layer to single output\n",
    "\n",
    "    #define the forward propagation/prediction equation of our model\n",
    "    def forward(self, x):\n",
    "        x = self.h1(x) #linear combination\n",
    "        x = F.relu(x) #activation\n",
    "        x = self.out(x) #linear combination\n",
    "        x = F.sigmoid(x) #activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training hyperparameters, costfunction and optimizer. Instantiate a model from the class we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyper-parameters\n",
    "no_epochs = 150 #number of epochs to train for\n",
    "lr = 0.0005 #learning rate\n",
    "\n",
    "mynet = Net() #instantiate model from class\n",
    "criterion = torch.nn.BCELoss() #define cost criterion\n",
    "optimizer = torch.optim.Rprop(mynet.parameters(), lr=lr) #choose optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the axes which we will use to plot our loss function as we train. <br>\n",
    "Define the function which we will use to train our model and train for the number of epochs specified by the no_epochs variable earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlim(0, no_epochs)\n",
    "\n",
    "def train(no_epochs, lr):\n",
    "    for epoch in range(no_epochs):\n",
    "        #forward propagate - calulate our hypothesis\n",
    "        h = mynet.forward(x_train)\n",
    "\n",
    "        #calculate, plot and print cost\n",
    "        cost = criterion(h, y_train)\n",
    "        costs.append(cost.data[0])\n",
    "        ax.plot(costs, 'b')\n",
    "        fig.canvas.draw()\n",
    "        print('Epoch ', epoch+1, ' Cost: ', cost.data[0])\n",
    "\n",
    "        #backpropagate + gradient descent step\n",
    "        optimizer.zero_grad() #set gradients to zero\n",
    "        cost.backward() #backpropagate to calculate derivatives\n",
    "        optimizer.step() #update our weights\n",
    "\n",
    "train(no_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calulate the accuracy of our model on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9663865546218487\n"
     ]
    }
   ],
   "source": [
    "test_h = mynet.forward(x_test) #predict values for out test set\n",
    "test_h.data.round_() #round output probabilities to give us discrete predictions\n",
    "correct = test_h.data.eq(y_test.data) #perform element-wise equality operation\n",
    "accuracy = torch.sum(correct)/correct.shape[0] #calculate accuracy\n",
    "print('Test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the optimized model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mynet.state_dict(), 'mynet_trained')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
