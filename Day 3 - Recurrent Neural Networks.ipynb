{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "So far, the neural network architectures we have been using have been simple in the sense that they take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words? Another issue is that each output is only dependent on the current input. It has no 'memory' of previous inputs. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state is calculated as a function of the previous hidden state and the current input. This hidden state can then be used to represent your output or can be put through another function to compute the outputs. When we say function we are referring to the same one used in standard neural network: linear combination followed by an activation function.\n",
    "\n",
    "### $h_t = f(x_t, h_{t-1})$\n",
    "\n",
    "As shown in the diagram below, which uses a further function to compute the output $o$ from the hidden state $s$, there are three matrices of parameters which we are trying to optimize: U, V and W. The diagram also demonstrates how these networks can be unfolded to show the variables at various time steps.\n",
    "\n",
    "![](rnn.jpg)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![](rnnlayouts.jpeg)\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character based on the 'memory' stored in the hidden units of all the previous characters.\n",
    "\n",
    "As always, we begin by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular task, we will need to do quite a bit of pre-processing. We need to find the number of unique characters in our training text and give each one a unique number so we can one-hot encode them.<br>\n",
    "We start by reading the file, converting all letters to lowercase to reduce the number of characters we need to model, then defining a function which takes in the text and gives up back a dictionary mapping each letter to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open our text file and read all the data into the rawtxt variable\n",
    "with open('lyrics', 'r') as file:\n",
    "    rawtxt = file.read()\n",
    "\n",
    "#turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "rawtxt = rawtxt.lower()\n",
    "\n",
    "#returns a dictionary that allows us to map from a unique number to a unique character in our text\n",
    "def create_map(rawtxt):\n",
    "    \n",
    "    letters = list(set(rawtxt)) #returns the list of unique characters in our raw text\n",
    "    lettermap = dict(enumerate(letters)) #created the dictionary mapping\n",
    "\n",
    "    return lettermap\n",
    "\n",
    "num_to_let = create_map(rawtxt) #store the dictionary mapping from numbers to characters in a variable\n",
    "let_to_num = dict(zip(num_to_let.values(), num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "\n",
    "nchars = len(num_to_let) #number of unique characters in our text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function which takes in text and a dictionary and maps each character in the text to the value specified for it in the dictionary. We then use this to map all of our text into the unique numbers for each character so it can be used with our RNN model. The labels are specified as the input but shifted by one time step as the label for each input is the character which comes after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maparray(txt, mapdict):\n",
    "    \n",
    "    txt = list(txt)\n",
    "\n",
    "    #iterate through our text and change the value for each character to its mapped value\n",
    "    for k, letter in enumerate(txt):\n",
    "        txt[k] = mapdict[letter]\n",
    "\n",
    "    txt = np.array(txt)\n",
    "    return txt\n",
    "\n",
    "#map our raw text into our input variables using the function defined earlier and passing in the mapping from letters to numbers\n",
    "X = maparray(rawtxt, let_to_num)\n",
    "Y = np.roll(X, -1, axis=0) #our label is the next character so roll shifts our array by one timestep\n",
    "\n",
    "#conver to torch tensors so we can use them in our torch model\n",
    "X = torch.LongTensor(X)\n",
    "Y = torch.LongTensor(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.<br>\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, input_size)\n",
    "        self.rnn = torch.nn.GRU(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x.view(1, -1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(x.view(1, 1, -1), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(1, -1)) #calculate our output based on output of rnn\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size)) #initialize our hidden state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks.<br>\n",
    "We also define the function which return a random chunk from the text which we can use to train out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "no_epochs = 20\n",
    "chunk_size = 100 #the length of the sequences which we will optimize over\n",
    "\n",
    "myrnn = RNN(nchars, 512, nchars, 1) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.CrossEntropyLoss() #define our cost function\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "#return a random batch for training\n",
    "def random_chunk(chunk_size):\n",
    "    k = np.random.randint(0, len(X)-chunk_size)\n",
    "    return X[k:k+chunk_size], Y[k:k+chunk_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the axes for plotting our cost per epoch. Define the training loop, sequentially feeding in a random chunk of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting costs\n",
    "costs = []\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlim(0, no_epochs-1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#training loop\n",
    "def train(no_epochs):\n",
    "    for epoch in range(no_epochs):\n",
    "        totcost = 0 #stores the cost per epoch\n",
    "        generated = '' #stores the text generated by our model each epoch\n",
    "        #given our chunk size, how many chunks do we need to optimize over to have gone thorough our whole dataset\n",
    "        for _ in range(len(X)//chunk_size):\n",
    "            h = myrnn.init_hidden() #initialize our hidden state to 0s\n",
    "            cost = 0 #cost for this chunk\n",
    "            x, y = random_chunk(chunk_size) #get a random sequence chunk to train\n",
    "            x, y = Variable(x), Variable(y) #turn into variables to be used with our model\n",
    "            #sequentially input each character in our sequence and calculate loss\n",
    "            for i in range(chunk_size):\n",
    "                out, h = myrnn.forward(x[i], h) #calculate outputs based on input and previous hidden state\n",
    "\n",
    "                #based on our output, what character does our network predict is next?\n",
    "                _, outl = out.data.max(1) \n",
    "                letter = num_to_let[outl[0]]\n",
    "                generated+=letter #add the predicted letter to our generated sequence\n",
    "\n",
    "                cost += criterion(out, y[i]) #add the cost for this input to the cost for this current chunk\n",
    "\n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            totcost+=cost.data[0] #add the cost of this sequence to the cost of this epoch\n",
    "        totcost /= len(X)//chunk_size #divide by the number of chunks per epoch to get average cost per epoch\n",
    "\n",
    "        #append the cost to the array and plot\n",
    "        costs.append(totcost)\n",
    "        ax.plot(costs, 'b')\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg cost/chunk: ', totcost)\n",
    "        print('Generated text: ', generated[0:750], '\\n')\n",
    "        \n",
    "train(no_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be ur every and your stitch inside aye\n",
      "watchin' all the snakes, curvin' all the fakes\n",
      "phone neve a sound like poetic justice\n",
      "if i told you that a flower for you outta cotton, i jeselverse bask in sin?\n",
      "pass the gin, i mix it what the fuck you heard\n",
      "and pessimists never struck my seetion stackin' up the go\n",
      "and sleepin' in a villa\n",
      "sippin' from again, then win all a this how in is when i'm walking down thess we hood see my enemies and critics layalty, got royalty inside my dna\n",
      "power shows inside my dna\n",
      "dna\n",
      "gimme some ganja, gimme some ganja\n",
      "real nigga in my dna\n",
      "ain't no ho inside my dna\n",
      "dnappy hopean a tiffor, lovant they survival of resentment\n",
      "resentment that sundress, ooh\n",
      "good god, what you doing hail in office\n",
      "we lost barack and crown vic, my memory been gone since\n",
      "donâ€™t ask about my foes\n",
      "'less you askin' me abomate\n",
      "i don't fabricate it, aye\n",
      "most of ya'll be faking, aye\n",
      "i stay modest but your pat you don't rapper like pyeaks, aye\n",
      "i don't fabricate it, aye\n",
      "most of ya'll be bark as bot baby in a spiral\n",
      "soprano c, we cotrest, or do we bask in sin?\n",
      "pass the gin, i mix it was walkin'?\n",
      "now i run the game, got the whole world talkin' (king kunta)\n",
      "everybody wanna cut the legs off him\n",
      "\n",
      "you goat mouthin' mamma's couch in polo socks, aye\n",
      "this shit way too crazy, aye\n",
      "you do not amaze me, aye\n",
      "i don't fabricate it, aye\n",
      "most of ya'll be faking, i could do it\n",
      "maybe one day when you figure out it's alright here in they say communication save relations, i can tell\n",
      "but i can never right a blue stat\n"
     ]
    }
   ],
   "source": [
    "def generate(prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated = prime_str\n",
    "    \n",
    "    #initialize hidden state\n",
    "    h = myrnn.init_hidden()\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num)\n",
    "    x = Variable(torch.LongTensor(prime_str))\n",
    "    \n",
    "    #primes our hidden state with the input string\n",
    "    for i in range(len(x)):\n",
    "        out, h = myrnn.forward(x[i], h)\n",
    "    \n",
    "    x = x[-1]\n",
    "    \n",
    "    for i in range(str_len):\n",
    "        out, h = myrnn.forward(x, h)\n",
    "        \n",
    "        out_dist = out.data.view(-1).div(temperature).exp()\n",
    "        sample = torch.multinomial(out_dist, 1)[0]\n",
    "        pred_char = num_to_let[sample]\n",
    "        \n",
    "        generated += pred_char\n",
    "        \n",
    "        x = Variable(torch.LongTensor([sample]))\n",
    "    \n",
    "    return generated\n",
    "        \n",
    "    \n",
    "\n",
    "gen = generate('this be ', 1500, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
